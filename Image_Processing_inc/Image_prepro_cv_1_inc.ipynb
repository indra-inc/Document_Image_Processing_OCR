{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "from PIL import Image\n",
    "# from pdf2image import convert_from_path\n",
    "# import pytesseract as pyt\n",
    "\n",
    "from skimage import color\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = './test_data/hos1.jpeg'\n",
    "samp2 = './test_data/form1.png'\n",
    "samp3 = './test_data/banking.jpg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource:\n",
    "- https://www.youtube.com/watch?v=ADV-AjAXHdc\n",
    "##### Task to be performed\n",
    "- Inverted Images\n",
    "- Rescaling\n",
    "- Binarization\n",
    "- Noise Removal\n",
    "- Dilation and Erosion\n",
    "- Rotation / Deskewing\n",
    "- Removing Borders\n",
    "- Missing Borders\n",
    "- Transparency / Alpha Channel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening Raw Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/Indranath.Chatterjee/ocr_demo/ocr-project-venv/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "## Opening Images\n",
    "imgRaw1 = cv2.imread(samp1)\n",
    "\n",
    "cv2.imshow('Sample-Raw-Image-1', imgRaw1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        ...,\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1]],\n",
       "\n",
       "       [[194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        ...,\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1]],\n",
       "\n",
       "       [[194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        ...,\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        ...,\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1]],\n",
       "\n",
       "       [[194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        ...,\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1]],\n",
       "\n",
       "       [[194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        ...,\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1],\n",
       "        [194, 232,   1]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgRaw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 555, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgRaw1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inverted Images   (white becomes black and vice versa)\n",
    "\n",
    "imgInverted1 = cv2.bitwise_not(imgRaw1)\n",
    "\n",
    "cv2.imshow('Inverted-Image-1', imgInverted1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- Therefore all the darker area will become light and all the light area becomes dark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization:\n",
    "- S1: First convert the image into Gray Scale \n",
    "- S2: Then convert Gray Scale into Binary that is Black and White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((692, 555, 3), numpy.ndarray, dtype('uint8'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgRaw1.shape, type(imgRaw1), imgRaw1.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GrayScaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Grayscale conversion using PIL\n",
    "\n",
    "# img_gray_pil = Image.open(samp1).convert('L')\n",
    "# # print(type(img_gray_pil))\n",
    "# img_gray_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to grayscale using skimage\n",
    "# plt.figure(figsize=(15,12))\n",
    "# img_gry_skim = color.rgb2gray(img1)\n",
    "# io.imshow(img_gry_skim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GrayScaling using cv2\n",
    "\n",
    "imgGrayCV1 = cv2.cvtColor(imgRaw1, cv2.COLOR_BGR2GRAY) \n",
    "cv2.imshow('Gray-Scale-Image-1', imgGrayCV1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), numpy.ndarray, (692, 555))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgGrayCV1.dtype, type(imgGrayCV1), imgGrayCV1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary-OTSU Thresholding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold the image to create a binary image  using Otsuâ€™s Binarization\n",
    "\n",
    "thresh, imgBinary_thr_1 = cv2.threshold(imgGrayCV1, 170, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "cv2.imshow('Binary-Image-1-OTSU', imgBinary_thr_1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- cv2.THRESH_BINARY | cv2.THRESH_OTSU: This method performs a bitwise OR operation between the binary threshold and the Otsu's threshold. In other words, it applies binary thresholding and Otsu's thresholding simultaneously, and the resulting image is the combination of both. Pixels with intensity values higher than the threshold determined by Otsu's method will be set to 255, while pixels with lower intensity values will be set to 0.\n",
    "\n",
    "- cv2.THRESH_BINARY + cv2.THRESH_OTSU: This method applies binary thresholding first and then adds the Otsu's threshold value to the resulting image. This means that it applies binary thresholding to the input image, and then adds the threshold value determined by Otsu's method to all the pixel values in the resulting image. As a result, the pixels with intensity values higher than the threshold determined by Otsu's method will be set to a higher value than 255, while pixels with lower intensity values will be set to 0.\n",
    "\n",
    "- The thresholding operation will convert pixel values in the grayscale image that are below the threshold value of 50 to 0 and those above it to 500. The cv2.THRESH_BINARY flag is used to set the output to either 0 or 500.\n",
    "\n",
    "- The cv2.THRESH_OTSU flag is used to automatically determine an optimal threshold value using Otsu's method. This method calculates a threshold value that minimizes the intra-class variance of the pixel values, essentially finding a value that best separates the foreground and background of the image.\n",
    "\n",
    "- The function cv2.threshold returns a tuple of two values: the threshold value that was used (in this case, determined automatically using Otsu's method), and the thresholded image.\n",
    "\n",
    "- So the output tuple will be: (threshold_value, thresholded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175.0,\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh, imgBinary_thr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), numpy.ndarray, (692, 555))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgBinary_thr_1.dtype, type(imgBinary_thr_1), imgBinary_thr_1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary-Adaptive Thresholding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply adaptive thresholding with neighborhood pixels and C constant value \n",
    "\n",
    "imgAdaptv_thr_1 = cv2.adaptiveThreshold(imgGrayCV1, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                                      cv2.THRESH_BINARY, 151,11)\n",
    "cv2.imshow('Binary-Image-1-Adaptive', imgAdaptv_thr_1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach-1 for Noise Removal\n",
    "# denoising of image saving it into dst image \n",
    "\n",
    "imgDenoise1 = cv2.fastNlMeansDenoising(imgGrayCV1, None, 10, 7, 21)\n",
    "\n",
    "cv2.imshow('Noise-Removal-1', imgDenoise1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "## cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21) for color image\n",
    "## cv2.fastNlMeansDenoising(image, None, 10, 7, 21) for gray or BW image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-local Means denoising using cv2.fastNlMeansDenoising() with the following parameters:\n",
    "\n",
    "    - src: the input image to be denoised\n",
    "    - dst: output image\n",
    "    - h: parameter regulating filter strength\n",
    "    - hForColorComponents: same as h, but for color images only\n",
    "    - templateWindowSize: Size of the window used to search for pixels with similar color\n",
    "    - searchWindowSize: Size of the window used to search for pixels with similar color in\n",
    "    each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 2 for Noise Removal\n",
    "\n",
    "def noiseRemoval(image):\n",
    "    \n",
    "    ## define structuring element i.e creating kernel matrix\n",
    "    # way-1\n",
    "    # kernel = np.ones((1,1), np.uint8)     ## creating kernel matrix\n",
    "\n",
    "    # way-2\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))   ## creating 1*1 kernel matrix\n",
    "\n",
    "\n",
    "    ## Performing Closing operation\n",
    "\n",
    "    image = cv2.dilate(image, kernel, iterations=1)\n",
    "    image = cv2.erode(image, kernel, iterations=1)\n",
    "\n",
    "    \n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)   # for closing operations\n",
    "    \n",
    "    image = cv2.medianBlur(image, 3)    ## median filtering where kernel = 3 and must be odd number\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return (image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDenoise2 = noiseRemoval(imgGrayCV1)\n",
    "cv2.imshow('Noise-Removal-2', imgDenoise2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "### cv2.dilate(): It is used to increase the size of foreground objects in an image and to fill in small gaps between them.\n",
    "\n",
    "     - It takes only 2 parameters:\n",
    "\n",
    "        - input image which can be ONLY GRAY or BW i.e Binary\n",
    "\n",
    "        - A structuring element, which defines the shape and size of the neighborhood over which the dilation operation is applied\n",
    "\n",
    "        - 'iterations' optional parameter that specifies the number of times the dilation operation is applied to the input image. Each iteration applies the dilation operation to the output of the previous iteration, resulting in progressively larger foreground objects. Increasing the value of iterations increases the size and connectedness of the foreground objects in the output image. However, applying too many iterations can also cause the foreground objects to merge together and lose their original shape.\n",
    "\n",
    "        - The iterations parameter is useful when performing morphological operations on images that have uneven lighting or when the foreground objects in the image have varying sizes. In such cases, applying multiple iterations can help to produce more consistent and robust results.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv2.erode(): \n",
    "\n",
    "        - used to reduce the size of foreground objects in an image and to remove small protrusions from them.\n",
    "        - parameters are same as explained above for dilate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv2.morphologyEx():\n",
    "- function takes as \n",
    "\n",
    "        - input a binary image (where each pixel is either black or white), \n",
    "\n",
    "        - a structuring element (which defines the shape and size of the operation), and \n",
    "        \n",
    "        - a type of morphological operation (Erosion/Dilation/Opening/Closing etc.) to perform."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv2.medianBlur():\n",
    "\n",
    "        - is a function in OpenCV that performs median filtering on an image. Median filtering is a type of non-linear image smoothing operation, which is useful for removing noise while preserving edges and fine details in the image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation and Deskewing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #https://becominghuman.ai/how-to-automatically-deskew-straighten-a-text-image-using-opencv-a0c30aed83df\n",
    "\n",
    "\n",
    "# def getSkewAngle(cvImage) -> float:\n",
    "#     # Prep image, copy, convert to gray scale, blur, and threshold\n",
    "#     newImage = cvImage.copy()\n",
    "#     gray = cv2.cvtColor(newImage, cv2.COLOR_BGR2GRAY)\n",
    "#     blur = cv2.GaussianBlur(gray, (9, 9), 0)\n",
    "#     thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "#     # Apply dilate to merge text into meaningful lines/paragraphs.\n",
    "#     # Use larger kernel on X axis to merge characters into single line, cancelling out any spaces.\n",
    "#     # But use smaller kernel on Y axis to separate between different blocks of text\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (30, 5))\n",
    "#     dilate = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "#     # Find all contours\n",
    "#     contours, hierarchy = cv2.findContours(dilate, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contours = sorted(contours, key = cv2.contourArea, reverse = True)\n",
    "#     for c in contours:\n",
    "#         rect = cv2.boundingRect(c)\n",
    "#         x,y,w,h = rect\n",
    "#         cv2.rectangle(newImage,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "#     # Find largest contour and surround in min area box\n",
    "#     largestContour = contours[0]\n",
    "#     print (len(contours))\n",
    "#     minAreaRect = cv2.minAreaRect(largestContour)\n",
    "#     cv2.imwrite(\"temp/boxes.jpg\", newImage)\n",
    "#     # Determine the angle. Convert it to the value that was originally used to obtain skewed image\n",
    "#     angle = minAreaRect[-1]\n",
    "#     if angle < -45:\n",
    "#         angle = 90 + angle\n",
    "#     return -1.0 * angle\n",
    "\n",
    "\n",
    "# # Rotate the image around its center\n",
    "\n",
    "# def rotateImage(cvImage, angle: float):\n",
    "#     newImage = cvImage.copy()\n",
    "#     (h, w) = newImage.shape[:2]\n",
    "#     center = (w // 2, h // 2)\n",
    "#     M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "#     newImage = cv2.warpAffine(newImage, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "#     return newImage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deskew image\n",
    "# def deskew(cvImage):\n",
    "#     angle = getSkewAngle(cvImage)\n",
    "#     return rotateImage(cvImage, -1.0 * angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## chatGPT\n",
    "\n",
    "# def skew_correction(image):\n",
    "\n",
    "#     # convert into grayscale image\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Invert the color of the image\n",
    "#     gray = cv2.bitwise_not(gray)\n",
    "\n",
    "#     # Thresolding the grayscale image using OTSU's Method to obtain binary image\n",
    "#     thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "#     # Extracting coordinates of non zero pixels from the binary image using np.where and stack them\n",
    "#     coords = np.column_stack(np.where(thresh > 0))\n",
    "\n",
    "#     # To get minimum bounding rectangle of the non zero pixels in the binary image\n",
    "#     # -1 index will return angle of rotation required to align the rectangle with horizontal axis\n",
    "#     angle = cv2.minAreaRect(coords)[-1]   \n",
    "\n",
    "#     if angle < -45:\n",
    "#         angle = -(90 + angle)\n",
    "#     else:\n",
    "#         angle = -angle\n",
    "\n",
    "#     (h, w) = image.shape[:2]\n",
    "#     center = (w // 2, h // 2)\n",
    "\n",
    "#     # use this angle to rotate the input image\n",
    "#     M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "#     rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "#     return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_borders(image):\n",
    "#     contours, heiarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     cntsSorted = sorted(contours, key=lambda x:cv2.contourArea(x))\n",
    "#     cnt = cntsSorted[-1]\n",
    "#     x, y, w, h = cv2.boundingRect(cnt)\n",
    "#     crop = image[y:y+h, x:x+w]\n",
    "#     return (crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## To find contour\n",
    "\n",
    "# # Load the image in grayscale\n",
    "# img = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # Apply edge detection to create a binary image\n",
    "# edges = cv2.Canny(img, 100, 200)\n",
    "\n",
    "# # Find contours in the binary image\n",
    "# contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Create a copy of the original image to draw the contours on\n",
    "# contour_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# # Draw the contours on the image\n",
    "# cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# # Display the original image and the image with contours\n",
    "# cv2.imshow('Original Image', img)\n",
    "# cv2.imshow('Contour Image', contour_img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Edge detection\n",
    "\n",
    "# # Load the image in grayscale\n",
    "# img = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # Apply Gaussian smoothing to reduce noise\n",
    "# blur = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "\n",
    "# # Apply Canny edge detection\n",
    "# edges = cv2.Canny(blur, 100, 200)  \n",
    "# image, the minimum and maximum threshold values, and optionally the aperture size\n",
    "\n",
    "# # Display the original image and the edge map\n",
    "# cv2.imshow('Original Image', img)\n",
    "# cv2.imshow('Edge Map', edges)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample_ocr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ee9332829e8c531245df2729c74b2dbe2d6ea4e8e840e77c9b63d537869ed57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
